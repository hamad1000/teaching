{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Group Names\n",
        "- Hamad Alqahtani\n",
        "- Samer Almontasheri\n",
        "- Ali Alghamdi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCUw45mZKjG"
      },
      "source": [
        "Intro to NLP Practical<br>\n",
        "======================<br>\n",
        "Students will work through problems on n-grams, probabilities, OOV handling, and classifiers.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJnSQ_vqZKjO"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2tr5e7WZKjQ"
      },
      "source": [
        "Toy corpus for language modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uJmCM0sZKjS"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"Its fleece was white as snow\",\n",
        "    \"And everywhere that Mary went\",\n",
        "    \"The lamb was sure to go\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nIvCzEeZKjT"
      },
      "source": [
        "--- Part 1: Preprocessing ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re-DkrvQZKjU"
      },
      "source": [
        " Q1.1 Sequence notation<br>\n",
        "Exercise: Write sequence notation for the sentence:<br>\n",
        "\"Mary had a little lamb, its fleece was white as snow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAOpzABuZKjV"
      },
      "source": [
        " Q1.2 Add start/end tokens<br>\n",
        "Exercise: Write a function to tokenize the corpus and add <s>, </s>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpuzFC7oEwSx",
        "outputId": "b9156f31-67f7-43e9-81e4-286c96d18110"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHls9GU_DEWL",
        "outputId": "b8a984b6-be44-4680-f6e6-c8e3134dd404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Mary', 'had', 'a', 'little', 'lamb', ',', 'its', 'fleece', 'was', 'white', 'as', 'snow']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Mary had a little lamb, its fleece was white as snow\"\n",
        "\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "print(tokenized_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm-QZvt6FgqU"
      },
      "outputs": [],
      "source": [
        "def add_start_end_tokens(corpus):\n",
        "    tokenized_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "    start_end_corpus = [['<s>'] + sentence + ['</s>'] for sentence in tokenized_corpus]\n",
        "    return start_end_corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KUFRwJGF4zq",
        "outputId": "9b114745-3603-4c1e-93ec-64241d752f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['<s>', 'Mary', 'had', 'a', 'little', 'lamb', '</s>'], ['<s>', 'Its', 'fleece', 'was', 'white', 'as', 'snow', '</s>'], ['<s>', 'And', 'everywhere', 'that', 'Mary', 'went', '</s>'], ['<s>', 'The', 'lamb', 'was', 'sure', 'to', 'go', '</s>']]\n"
          ]
        }
      ],
      "source": [
        "senx = add_start_end_tokens(corpus)\n",
        "print(senx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH1mpV5ZKjY"
      },
      "source": [
        "--- Part 2: N-grams & Probabilities ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xVR2UlsZKjZ"
      },
      "source": [
        " Q2.1 Extract unigrams, bigrams, trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmi4UmWXHxQN"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JpRgpHMGFbd"
      },
      "outputs": [],
      "source": [
        "# sentence = \"Mary had a little lamb, its fleece was white as snow\"   already have it tokenized\n",
        "\n",
        "uni = list(ngrams(tokenized_sentence, 1))\n",
        "bi = list(ngrams(tokenized_sentence, 2))\n",
        "tri = list(ngrams(tokenized_sentence, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuLfxVPiH0gA",
        "outputId": "ca6c047c-3fe7-407e-d5c4-503ebb550fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uni-grams: [('Mary',), ('had',), ('a',), ('little',), ('lamb',), (',',), ('its',), ('fleece',), ('was',), ('white',), ('as',), ('snow',)]\n",
            "Bi-grams: [('Mary', 'had'), ('had', 'a'), ('a', 'little'), ('little', 'lamb'), ('lamb', ','), (',', 'its'), ('its', 'fleece'), ('fleece', 'was'), ('was', 'white'), ('white', 'as'), ('as', 'snow')]\n",
            "Tri-grams: [('Mary', 'had', 'a'), ('had', 'a', 'little'), ('a', 'little', 'lamb'), ('little', 'lamb', ','), ('lamb', ',', 'its'), (',', 'its', 'fleece'), ('its', 'fleece', 'was'), ('fleece', 'was', 'white'), ('was', 'white', 'as'), ('white', 'as', 'snow')]\n"
          ]
        }
      ],
      "source": [
        "print(\"Uni-grams:\", uni)\n",
        "print(\"Bi-grams:\", bi)\n",
        "print(\"Tri-grams:\", tri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RuQzfXWZKjb"
      },
      "source": [
        " Q2.2 Bigram probabilities<br>\n",
        "Exercise: Write function to compute P(w_i | w_{i-1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_JCC71KIMsu",
        "outputId": "8905fd14-6d91-461b-dc83-d9ae9072a40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{('<s>', 'Mary'): 0.25, ('Mary', 'had'): 0.5, ('had', 'a'): 1.0, ('a', 'little'): 1.0, ('little', 'lamb'): 1.0, ('lamb', '</s>'): 0.5, ('<s>', 'Its'): 0.25, ('Its', 'fleece'): 1.0, ('fleece', 'was'): 1.0, ('was', 'white'): 0.5, ('white', 'as'): 1.0, ('as', 'snow'): 1.0, ('snow', '</s>'): 1.0, ('<s>', 'And'): 0.25, ('And', 'everywhere'): 1.0, ('everywhere', 'that'): 1.0, ('that', 'Mary'): 1.0, ('Mary', 'went'): 0.5, ('went', '</s>'): 1.0, ('<s>', 'The'): 0.25, ('The', 'lamb'): 1.0, ('lamb', 'was'): 0.5, ('was', 'sure'): 0.5, ('sure', 'to'): 1.0, ('to', 'go'): 1.0, ('go', '</s>'): 1.0}\n",
            "Counter({'<s>': 4, '</s>': 4, 'Mary': 2, 'lamb': 2, 'was': 2, 'had': 1, 'a': 1, 'little': 1, 'Its': 1, 'fleece': 1, 'white': 1, 'as': 1, 'snow': 1, 'And': 1, 'everywhere': 1, 'that': 1, 'went': 1, 'The': 1, 'sure': 1, 'to': 1, 'go': 1})\n"
          ]
        }
      ],
      "source": [
        "def bigram_probabilities(corpus):\n",
        "  token = [['<s>'] + word_tokenize(sent) + ['</s>'] for sent in corpus]\n",
        "  bigrams = [list(ngrams(word, 2)) for word in token]\n",
        "  bi_freq = Counter([gram for sublist in bigrams for gram in sublist])\n",
        "  uni_freq = Counter([word for sublist in token for word in sublist])\n",
        "\n",
        "  # P(wi​∣wi−1​)=Count(wi−1​)Count(wi−1​,wi​)​\n",
        "\n",
        "  bi_prob = {(w1, w2): count / uni_freq[w1] for (w1, w2), count in bi_freq.items()}\n",
        "  return bi_prob , uni_freq\n",
        "\n",
        "\n",
        "prob, uni_freq = bigram_probabilities(corpus)\n",
        "print(prob)\n",
        "print(uni_freq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3G1JGIAZKjj"
      },
      "source": [
        " Q2.3 Sentence probability<br>\n",
        "Exercise: Compute probability of \"Mary had a little lamb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVGQJb_CL1-G",
        "outputId": "4d059726-6a64-4c2a-e5e8-99664254c571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', 'Mary', 'had', 'a', 'little', 'lamb', '</s>']\n",
            "{('<s>', 'Mary'): 0.25, ('Mary', 'had'): 0.5, ('had', 'a'): 1.0, ('a', 'little'): 1.0, ('little', 'lamb'): 1.0, ('lamb', '</s>'): 0.5, ('<s>', 'Its'): 0.25, ('Its', 'fleece'): 1.0, ('fleece', 'was'): 1.0, ('was', 'white'): 0.5, ('white', 'as'): 1.0, ('as', 'snow'): 1.0, ('snow', '</s>'): 1.0, ('<s>', 'And'): 0.25, ('And', 'everywhere'): 1.0, ('everywhere', 'that'): 1.0, ('that', 'Mary'): 1.0, ('Mary', 'went'): 0.5, ('went', '</s>'): 1.0, ('<s>', 'The'): 0.25, ('The', 'lamb'): 1.0, ('lamb', 'was'): 0.5, ('was', 'sure'): 0.5, ('sure', 'to'): 1.0, ('to', 'go'): 1.0, ('go', '</s>'): 1.0}\n",
            "Counter({'<s>': 4, '</s>': 4, 'Mary': 2, 'lamb': 2, 'was': 2, 'had': 1, 'a': 1, 'little': 1, 'Its': 1, 'fleece': 1, 'white': 1, 'as': 1, 'snow': 1, 'And': 1, 'everywhere': 1, 'that': 1, 'went': 1, 'The': 1, 'sure': 1, 'to': 1, 'go': 1})\n",
            "7\n",
            "['Mary had a little lamb', 'Its fleece was white as snow', 'And everywhere that Mary went', 'The lamb was sure to go']\n",
            "0.0625\n"
          ]
        }
      ],
      "source": [
        "sen = \"Mary had a little lamb\"\n",
        "sen = add_start_end_tokens([sen])[0]\n",
        "print(sen)\n",
        "print(prob)\n",
        "print(uni_freq)\n",
        "print(len(sen))\n",
        "print(corpus)\n",
        "\n",
        "prob1 = 1.0\n",
        "for i in range(1, len(sen)):\n",
        "  pair = (sen[i-1], sen[i])\n",
        "  if pair in prob:\n",
        "    prob1 *= prob[pair]\n",
        "  else:\n",
        "    prob1 *= 0  # if unseen and no smoothing\n",
        "\n",
        "print(prob1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0unZilSZKjk"
      },
      "source": [
        "Q2.4 Handling OOV/UNK<br>\n",
        "Exercise: Replace unseen words with <UNK> and recompute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iseSLHFtRz-0",
        "outputId": "75f3edf8-9ba9-426a-bed4-8c2aea74cc8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus with <UNK> tokens:\n",
            "[['Mary', 'had', 'a', 'little', 'lamb'], ['Its', 'fleece', 'was', 'white', 'as', 'snow'], ['And', 'everywhere', 'that', 'Mary', 'went'], ['The', 'lamb', 'was', 'sure', 'to', 'go']]\n"
          ]
        }
      ],
      "source": [
        "def replace_oov_with_unk(corpus, vocabulary):\n",
        "    unk_corpus = []\n",
        "    for sentence in corpus:\n",
        "        unk_sentence = [word if word in vocabulary else '<UNK>' for word in word_tokenize(sentence)]\n",
        "        unk_corpus.append(unk_sentence)\n",
        "    return unk_corpus\n",
        "\n",
        "initial_vocabulary = set([word for sentence in corpus for word in word_tokenize(sentence)])\n",
        "\n",
        "\n",
        "unk_corpus = replace_oov_with_unk(corpus, initial_vocabulary)\n",
        "print(\"Corpus with <UNK> tokens:\")\n",
        "print(unk_corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evU5bff_ZKjl"
      },
      "source": [
        "--- Part 3: Classifier ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DxxnAKZKjm"
      },
      "source": [
        " Q3.1 Naive Bayes sentiment classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyx3z1RL58el"
      },
      "source": [
        "# 📽 Exercise 3.1: Sentiment Classification on toy dataset\n",
        "\n",
        "In this exercise, you will build a simple sentiment classification model that predicts whether a given sentence is **positive** or **negative**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✏️ Instructions:\n",
        "\n",
        "\n",
        "### 1️⃣ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "\n",
        "🚀 **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3fkQ5DUZKjm"
      },
      "outputs": [],
      "source": [
        "train_texts = [\n",
        "    \"I love my dog\",\n",
        "    \"This food is great\",\n",
        "    \"I hate waiting\",\n",
        "    \"The movie was boring\",\n",
        "    \"Happy with my phone\",\n",
        "    \"This is awful\"\n",
        "]\n",
        "train_labels = [\"pos\", \"pos\", \"neg\", \"neg\", \"pos\", \"neg\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq7Vj1ng1lw8"
      },
      "source": [
        "# 📽 Exercise 3.2: Movie Review Classification using Movies Review Corpus\n",
        "\n",
        "In this exercise, you will build a simple text classification model that predicts whether a given **movie review** is **positive** or **negative** using the **NLTK Movie Reviews Corpus**.\n",
        "\n",
        "This is a classical example of text classification at the **sentence level**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✏️ Instructions:\n",
        "\n",
        "### 1️⃣ Load the Data\n",
        "- Import the **Movie Reviews corpus** from **NLTK**.\n",
        "- Create a dataset where each example is a review and the label is either `'positive'` or `'negative'`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ Evaluate the Classifier\n",
        "- Use **accuracy** and a **classification report** to evaluate your model on the test set.\n",
        "- Think about: How well does the model perform? Which reviews are harder to classify?\n",
        "\n",
        "---\n",
        "\n",
        "✅ You are free to explore:\n",
        "- Trying different classifiers.\n",
        "- Visualizing the results (e.g., confusion matrix).\n",
        "\n",
        "---\n",
        "\n",
        "🚀 **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVYgyaWtZKju"
      },
      "source": [
        " Q3.3 Discussion: Why bigrams vs unigrams?<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv13VAlzZKju"
      },
      "source": [
        " Q3.4 Limitations of n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mIB4S3wZKju"
      },
      "source": [
        "--- Part 4: Wrap-up Reflection ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzsTXZsTZKjv"
      },
      "source": [
        " Discussion Questions<br>\n",
        "1. Why do we need <UNK> tokens?<br>\n",
        "2. Why start/end tokens?<br>\n",
        "3. Why not always use higher n-grams?<br>\n",
        "4. How do classifiers differ from language models?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
